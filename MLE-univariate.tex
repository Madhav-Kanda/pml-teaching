\documentclass[handout]{beamer}

\usetheme[progressbar=frametitle]{metropolis}
\usepackage{appendixnumberbeamer}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tcolorbox}
\definecolor{metropolisblue}{RGB}{39, 59, 94}



% Begin document
\begin{document}

% Title page
\title{Maximum Likelihood Estimation}
\subtitle{Univariate}
\author{Nipun Batra}
\date{\today}
\institute{IIT Gandhinagar}
\maketitle

% Section 1
\section{Introduction}

\begin{frame}{Univariate Normal Distribution}
The probability density function of a univariate normal distribution is given by:

\begin{equation}
f(x|\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
\end{equation}

Let us assume we have a dataset $D = \{x_1, x_2, \ldots, x_n\}$, where each $x_i$ is an independent sample from the above distribution. 
We want to estimate the parameters $\theta = \{\mu, \sigma\}$ from the data.

Our likelihood function is given by:
\begin{equation}
P(D|\theta) = \mathcal{L}(\mu, \sigma^2) = \prod_{i=1}^n f(x_i|\mu, \sigma^2)
\end{equation}


\end{frame}

\begin{frame}{Log Likelihood Function}
    Log-likelihood function:
    \begin{equation}
        \log \mathcal{L}(\mu, \sigma^2) = \sum_{i=1}^n \log f(x_i|\mu, \sigma^2)
    \end{equation}

    Simplifying the above equation, we get:
    \begin{align*}
        \log \mathcal{L}(\mu, \sigma^2) &= \sum_{i=1}^n \log f(x_i|\mu, \sigma^2) \\
        &= \sum_{i=1}^n \log \left( \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{(x_i-\mu)^2}{2\sigma^2} \right) \right) \\
        &= \sum_{i=1}^n \left( \log \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right) + \log \left( \exp \left( -\frac{(x_i-\mu)^2}{2\sigma^2} \right) \right) \right) \\
        \end{align*}
\end{frame}

\begin{frame}
   
    \begin{align*}
        \log \mathcal{L}(\mu, \sigma^2) &= \sum_{i=1}^n \left( \log \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right) -\frac{(x_i-\mu)^2}{2\sigma^2} \right) \\
        &= \sum_{i=1}^n \left( -\frac{1}{2} \log (2\pi\sigma^2) -\frac{(x_i-\mu)^2}{2\sigma^2} \right) \\
        &= -\frac{n}{2} \log (2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i-\mu)^2
        \end{align*}

        % Put final equation in a box
        \begin{tcolorbox}[colback=metropolisblue!5,colframe=metropolisblue,title=Log Likelihood Function for Univariate Normal Distribution]
            Log-likelihood function for normally distributed data is given by:
            \begin{equation*}
                \log \mathcal{L}(\mu, \sigma^2) = -\frac{n}{2} \log (2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i-\mu)^2
            \end{equation*}
        \end{tcolorbox}
\end{frame}


\begin{frame}
    \frametitle{Maximum Likelihood Estimate for $\mu$}
    
    To find the MLE for $\mu$, we differentiate the log-likelihood function with respect to $\mu$ and set it to zero:
    
    \begin{align*}
        \frac{\partial \log \mathcal{L}(\mu, \sigma^2)}{\partial \mu} &= \frac{\partial}{\partial \mu} \left(-\frac{n}{2} \log (2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i-\mu)^2\right) =0\\
        \frac{\partial}{\partial \mu} \left(\sum_{i=1}^n (x_i-\mu)^2\right) &= 0
    \end{align*}
    
    \begin{tcolorbox}[colback=metropolisblue!5,colframe=metropolisblue,title=Maximum Likelihood Estimate for $\mu$]
        MLE of $\mu$, denoted as $\hat{\mu}_{\text{MLE}}$, is given by:
        \begin{equation*}
            \hat{\mu}_{\text{MLE}} = \frac{1}{n}\sum_{i=1}^n x_i
        \end{equation*}
    \end{tcolorbox}
    
    \end{frame}

\begin{frame}{MLE for $\sigma$ for normally distributed data}
    Recall that the log-likelihood function is given by:
    \begin{equation}
        \log \mathcal{L}(\mu, \sigma^2) = \sum_{i=1}^n \log f(x_i|\mu, \sigma^2)
    \end{equation}

    Let us find the maximum likelihood estimate of $\sigma^2$ now. We can do this by taking the derivative of the log-likelihood function with respect to $\sigma^2$ and equating it to zero.   

    \begin{equation}
        \frac{\partial \log \mathcal{L}(\mu, \sigma^2)}{\partial \sigma^2} = \sum_{i=1}^n \frac{\partial \log f(x_i|\mu, \sigma^2)}{\partial \sigma^2} = 0
    \end{equation}
    
\end{frame}

\begin{frame}{MLE for $\sigma$ for normally distributed data}
    Continuing from the previous equation, we can simplify the expression by substituting $t = \sigma^2$:

    \begin{align*}
        \frac{\partial \log \mathcal{L}(\mu, \sigma^2)}{\partial \sigma^2} &= \sum_{i=1}^n \frac{\partial \log f(x_i|\mu, \sigma^2)}{\partial \sigma^2} \\
        &= \sum_{i=1}^n \frac{\partial}{\partial \sigma^2}\left(-\frac{(x_i-\mu)^2}{2\sigma^2}\right) \\
        &= \sum_{i=1}^n \frac{(x_i-\mu)^2}{2(\sigma^2)^2} - \frac{1}{2\sigma^2}
    \end{align*}

    Substituting $t = \sigma^2$, we have:

    \begin{align*}
        \frac{\partial \log \mathcal{L}(\mu, t)}{\partial t} &= \sum_{i=1}^n \frac{(x_i-\mu)^2}{2t^2} - \frac{1}{2t}
    \end{align*}

    To find the maximum likelihood estimate of $\sigma^2$, we set the derivative equal to zero:

    \begin{equation*}
        \sum_{i=1}^n \frac{(x_i-\mu)^2}{2t^2} - \frac{1}{2t} = 0
    \end{equation*}
    
    Solving for $t$, we get:

    \begin{equation*}
        \hat{t}_{\text{MLE}} = \frac{1}{n} \sum_{i=1}^n (x_i - \mu)^2
    \end{equation*}

    Finally, the maximum likelihood estimate of $\sigma^2$, denoted as $\hat{\sigma}^2_{\text{MLE}}$, is obtained by substituting back $t = \hat{t}_{\text{MLE}}$ into the expression.
\end{frame}


    

\end{document}