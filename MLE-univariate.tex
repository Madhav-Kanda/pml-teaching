\documentclass[handout]{beamer}

\usetheme[progressbar=frametitle]{metropolis}
\usepackage{appendixnumberbeamer}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tcolorbox}
\definecolor{metropolisblue}{RGB}{39, 59, 94}



% Begin document
\begin{document}

% Title page
\title{Maximum Likelihood Estimation}
\subtitle{Univariate}
\author{Nipun Batra}
\date{\today}
\institute{IIT Gandhinagar}
\maketitle

% Section 1
\section{Introduction}

\begin{frame}{Univariate Normal Distribution}
The probability density function of a univariate normal distribution is given by:

\begin{equation}
f(x|\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
\end{equation}

Let us assume we have a dataset $D = \{x_1, x_2, \ldots, x_n\}$, where each $x_i$ is an independent sample from the above distribution. 
We want to estimate the parameters $\theta = \{\mu, \sigma\}$ from the data.

Our likelihood function is given by:
\begin{equation}
P(D|\theta) = \mathcal{L}(\mu, \sigma^2) = \prod_{i=1}^n f(x_i|\mu, \sigma^2)
\end{equation}


\end{frame}

\begin{frame}{Log Likelihood Function}
    Log-likelihood function:
    \begin{equation}
        \log \mathcal{L}(\mu, \sigma^2) = \sum_{i=1}^n \log f(x_i|\mu, \sigma^2)
    \end{equation}

    Simplifying the above equation, we get:
    \begin{align*}
        \log \mathcal{L}(\mu, \sigma^2) &= \sum_{i=1}^n \log f(x_i|\mu, \sigma^2) \\
        &= \sum_{i=1}^n \log \left( \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{(x_i-\mu)^2}{2\sigma^2} \right) \right) \\
        &= \sum_{i=1}^n \left( \log \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right) + \log \left( \exp \left( -\frac{(x_i-\mu)^2}{2\sigma^2} \right) \right) \right) \\
        \end{align*}
\end{frame}

\begin{frame}
   
    \begin{align*}
        \log \mathcal{L}(\mu, \sigma^2) &= \sum_{i=1}^n \left( \log \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right) -\frac{(x_i-\mu)^2}{2\sigma^2} \right) \\
        &= \sum_{i=1}^n \left( -\frac{1}{2} \log (2\pi\sigma^2) -\frac{(x_i-\mu)^2}{2\sigma^2} \right) \\
        &= -\frac{n}{2} \log (2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i-\mu)^2
        \end{align*}
        \begin{tcolorbox}[colback=metropolisblue!5,colframe=metropolisblue,title=Log Likelihood Function for Univariate Normal Distribution]
            Log-likelihood function for normally distributed data is:
            \[
                \log \mathcal{L}(\mu, \sigma^2) = -\frac{n}{2} \log(2\pi) - n\log(\sigma) - \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i-\mu)^2
                \]
        \end{tcolorbox}
\end{frame}








\begin{frame}
    \frametitle{Maximum Likelihood Estimate for $\mu$}
    
    To find the MLE for $\mu$, we differentiate the log-likelihood function with respect to $\mu$ and set it to zero:
    
    \begin{align*}
        \frac{\partial \log \mathcal{L}(\mu, \sigma^2)}{\partial \mu} &= \frac{\partial}{\partial \mu} \left(-\frac{n}{2} \log (2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i-\mu)^2\right) =0\\
        \frac{\partial}{\partial \mu} \left(\sum_{i=1}^n (x_i-\mu)^2\right) &= 0
    \end{align*}
    
    \begin{tcolorbox}[colback=metropolisblue!5,colframe=metropolisblue,title=Maximum Likelihood Estimate for $\mu$]
        MLE of $\mu$, denoted as $\hat{\mu}_{\text{MLE}}$, is given by:
        \begin{equation*}
            \hat{\mu}_{\text{MLE}} = \frac{1}{n}\sum_{i=1}^n x_i
        \end{equation*}
    \end{tcolorbox}
    
    \end{frame}

\begin{frame}{MLE for $\sigma$ for normally distributed data}
    Recall that the log-likelihood function is given by:
    \begin{equation}
        \log \mathcal{L}(\mu, \sigma^2) = \sum_{i=1}^n \log f(x_i|\mu, \sigma^2)
    \end{equation}

    Let us find the maximum likelihood estimate of $\sigma^2$ now. We can do this by taking the derivative of the log-likelihood function with respect to $\sigma^2$ and equating it to zero.   

    \begin{equation}
        \frac{\partial \log \mathcal{L}(\mu, \sigma^2)}{\partial \sigma^2} = \sum_{i=1}^n \frac{\partial \log f(x_i|\mu, \sigma^2)}{\partial \sigma^2} = 0
    \end{equation}
    
\end{frame}

\begin{frame}{MLE for $\sigma$ for normally distributed data}
    \begin{tcolorbox}[colback=metropolisblue!5,colframe=metropolisblue,title=Log Likelihood Function for Univariate Normal Distribution]
        Log-likelihood function for normally distributed data is:
        \[
            \log \mathcal{L}(\mu, \sigma^2) = -\frac{n}{2} \log(2\pi) - n\log(\sigma) - \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i-\mu)^2
            \]
    \end{tcolorbox}

Now, we can differentiate the log-likelihood function with respect to $\sigma$ and equate it to zero.
\end{frame}

\begin{frame}{MLE for $\sigma$ for normally distributed data}

    
        \[
        \frac{{\partial}}{{\partial \sigma}} \log \mathcal{L}(\mu, \sigma^2) = -\frac{n}{\sigma} + \frac{1}{\sigma^3} \sum_{i=1}^n (x_i-\mu)^2 = 0
        \]
    
        Multiplying through by $\sigma^3$, we have:
    
        \[
        -n \sigma^2 + \sum_{i=1}^n (x_i-\mu)^2 = 0
        \]
    
        \begin{tcolorbox}[colback=metropolisblue!5,colframe=metropolisblue,title=Maximum Likelihood Estimate for $\sigma^2$]
            MLE of $\sigma^2$, denoted as $\hat{\sigma}^2_{\text{MLE}}$, is given by:
            \[
                \sigma^2 = \frac{1}{n} \sum_{i=1}^n (x_i-\mu)^2
                \]
        \end{tcolorbox}
    
       

    \end{frame}
    




    

\end{document}